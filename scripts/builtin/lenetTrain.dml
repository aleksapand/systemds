

# Imports
source("nn/layers/affine.dml") as affine
source("nn/layers/conv2d_builtin.dml") as conv2d
source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("nn/layers/dropout.dml") as dropout
source("nn/layers/l2_reg.dml") as l2_reg
source("nn/layers/max_pool2d_builtin.dml") as max_pool2d
source("nn/layers/relu.dml") as relu
source("nn/layers/softmax.dml") as softmax
source("nn/optim/sgd_nesterov.dml") as sgd_nesterov
source("nn/layers/lenetForwardPass.dml") as lenet_fw

m_lenetTrain = function(matrix[double] X, matrix[double] Y,
                 matrix[double] X_val, matrix[double] Y_val,
                 int C, int Hin, int Win, int batch_size = 64, int epochs=20, 
                 int lr=0.01, int mu=0.9, int decay=0.95, int seed=-1)
    return (matrix[double] W1, matrix[double] b1,
            matrix[double] W2, matrix[double] b2,
            matrix[double] W3, matrix[double] b3,
            matrix[double] W4, matrix[double] b4) {
   /*
   * Trains a convolutional net using the "LeNet" architecture.
   *
   * The input matrix, X, has N examples, each represented as a 3D
   * volume unrolled into a single vector.  The targets, Y, have K
   * classes, and are one-hot encoded.
   *
   * Inputs:
   *  - X: Input data matrix, of shape (N, C*Hin*Win).
   *  - Y: Target matrix, of shape (N, K).
   *  - X_val: Input validation data matrix, of shape (N, C*Hin*Win).
   *  - Y_val: Target validation matrix, of shape (N, K).
   *  - C: Number of input channels (dimensionality of input depth).
   *  - Hin: Input height.
   *  - Win: Input width.
   *  - epochs: Total number of full training loops over the full data set.
   *  - lr: 
   *  - mu: 
   *  - decay: 
   *
   * Outputs:
   *  - W1: 1st layer weights (parameters) matrix, of shape (F1, C*Hf*Wf).
   *  - b1: 1st layer biases vector, of shape (F1, 1).
   *  - W2: 2nd layer weights (parameters) matrix, of shape (F2, F1*Hf*Wf).
   *  - b2: 2nd layer biases vector, of shape (F2, 1).
   *  - W3: 3rd layer weights (parameters) matrix, of shape (F2*(Hin/4)*(Win/4), N3).
   *  - b3: 3rd layer biases vector, of shape (1, N3).
   *  - W4: 4th layer weights (parameters) matrix, of shape (N3, K).
   *  - b4: 4th layer biases vector, of shape (1, K).
   */

  N = nrow(X)
  K = ncol(Y)

  # Create network:
  # conv1 -> relu1 -> pool1 -> conv2 -> relu2 -> pool2 -> affine3 -> relu3 -> affine4 -> softmax
  Hf = 5  # filter height
  Wf = 5  # filter width
  stride = 1
  pad = 2  # For same dimensions, (Hf - stride) / 2

  F1 = 32  # num conv filters in conv1
  F2 = 64  # num conv filters in conv2
  N3 = 512  # num nodes in affine3
  # Note: affine4 has K nodes, which is equal to the number of target dimensions (num classes)

  [W1, b1] = conv2d::init(F1, C, Hf, Wf, -1)  # inputs: (N, C*Hin*Win)
  [W2, b2] = conv2d::init(F2, F1, Hf, Wf, -1)  # inputs: (N, F1*(Hin/2)*(Win/2))
  [W3, b3] = affine::init(F2*(Hin/2/2)*(Win/2/2), N3, -1)  # inputs: (N, F2*(Hin/2/2)*(Win/2/2))
  [W4, b4] = affine::init(N3, K, -1)  # inputs: (N, N3)
  W4 = W4 / sqrt(2)  # different initialization, since being fed into softmax, instead of relu

  # Initialize SGD w/ Nesterov momentum optimizer
  
  # parameters now in function

  # lr = 0.01  # learning rate
  # mu = 0.9  #0.5  # momentum
  # decay = 0.95  # learning rate decay constant
  
  
  vW1 = sgd_nesterov::init(W1); vb1 = sgd_nesterov::init(b1)
  vW2 = sgd_nesterov::init(W2); vb2 = sgd_nesterov::init(b2)
  vW3 = sgd_nesterov::init(W3); vb3 = sgd_nesterov::init(b3)
  vW4 = sgd_nesterov::init(W4); vb4 = sgd_nesterov::init(b4)

  # Regularization
  lambda = 5e-04

  # Optimize
   if(param_server) {
        model = list(C=C, Hin=Hin, Win=Win, W1=W1, b1=b1, W2=W2, b2=b2, W3=W3, b3=b3, W4=W4, b4=b4)
        params = list(lr=0.001)
        model_trained = paramserv(model=model, features=X, labels=y, upd="gradients", agg="aggregation", epochs=1, hyperparams=params)
    } else {  
        iters = ceil(N / batch_size)
        for (e in 1:epochs) {
        for(i in 1:iters) {
        # Get next batch
        beg = ((i-1) * batch_size) %% N + 1
        end = min(N, beg + batch_size - 1)
        X_batch = X[beg:end,]
        y_batch = Y[beg:end,]

        [outc1, Houtc1, Woutc1, outr1, outp1, 
        Houtp1, Woutp1, outc2, Houtc2, Woutc2, 
        outr2, outp2, Houtp2, Woutp2, 
        outa3, outr3, outd3, maskd3, outa4, probs] = lenet_fw::lenetForward(X_batch, C, Hin, Win, W1, b1, W2, b2, W3, b3, W4, b4)

        # Compute loss & accuracy for training & validation data every 100 iterations.
        if (i %% 100 == 0) {
                # Compute training loss & accuracy
                loss_data = cross_entropy_loss::forward(probs, y_batch)
                loss_reg_W1 = l2_reg::forward(W1, lambda)
                loss_reg_W2 = l2_reg::forward(W2, lambda)
                loss_reg_W3 = l2_reg::forward(W3, lambda)
                loss_reg_W4 = l2_reg::forward(W4, lambda)
                loss = loss_data + loss_reg_W1 + loss_reg_W2 + loss_reg_W3 + loss_reg_W4
                accuracy = mean(rowIndexMax(probs) == rowIndexMax(y_batch))

                # Compute validation loss & accuracy
                probs_val = predict(X_val, C, Hin, Win, W1, b1, W2, b2, W3, b3, W4, b4)
                loss_val = cross_entropy_loss::forward(probs_val, Y_val)
                accuracy_val = mean(rowIndexMax(probs_val) == rowIndexMax(Y_val))

                # Output results
                print("Epoch: " + e + ", Iter: " + i + ", Train Loss: " + loss + ", Train Accuracy: "
                + accuracy + ", Val Loss: " + loss_val + ", Val Accuracy: " + accuracy_val)
        }

        # Compute data backward pass
        # call backward function here 

        [dW1, db1, dW2, db2, dW3, db3, dW4, db4] = feed_backward(X_batch, y_batch, C, Hin, Win, W1, b1, W2, b2, W3, b3, W4, b4, outc1, 
                                                                        Houtc1, Woutc1, outr1, outp1, 
                                                                        Houtp1, Woutp1, outc2, Houtc2, Woutc2, 
                                                                        outr2, outp2, Houtp2, Woutp2, 
                                                                        outa3, outr3, outd3, maskd3, outa4, probs)

        # Compute regularization backward pass
        dW1_reg = l2_reg::backward(W1, lambda)
        dW2_reg = l2_reg::backward(W2, lambda)
        dW3_reg = l2_reg::backward(W3, lambda)
        dW4_reg = l2_reg::backward(W4, lambda)
        dW1 = dW1 + dW1_reg
        dW2 = dW2 + dW2_reg
        dW3 = dW3 + dW3_reg
        dW4 = dW4 + dW4_reg

        # Optimize with SGD w/ Nesterov momentum
        [W1, vW1] = sgd_nesterov::update(W1, dW1, lr, mu, vW1)
        [b1, vb1] = sgd_nesterov::update(b1, db1, lr, mu, vb1)
        [W2, vW2] = sgd_nesterov::update(W2, dW2, lr, mu, vW2)
        [b2, vb2] = sgd_nesterov::update(b2, db2, lr, mu, vb2)
        [W3, vW3] = sgd_nesterov::update(W3, dW3, lr, mu, vW3)
        [b3, vb3] = sgd_nesterov::update(b3, db3, lr, mu, vb3)
        [W4, vW4] = sgd_nesterov::update(W4, dW4, lr, mu, vW4)
        [b4, vb4] = sgd_nesterov::update(b4, db4, lr, mu, vb4)
        }
        # Anneal momentum towards 0.999
        mu = mu + (0.999 - mu)/(1+epochs-e)
        # Decay learning rate
        lr = lr * decay
        }
   }


}

feed_backward = function(matrix[double] X, matrix[double] y, int C, int Hin, int Win,
                        matrix[double] W1, matrix[double] b1,
                        matrix[double] W2, matrix[double] b2,
                        matrix[double] W3, matrix[double] b3,
                        matrix[double] W4, matrix[double] b4,
                        matrix[double] outc1, int Houtc1, int Woutc1, matrix[double] outr1,
                        matrix[double] outp1, int Houtp1, int Woutp1,
                        matrix[double] outc2, int Houtc2, int Woutc2, matrix[double] outr2,
                        matrix[double] outp2, int Houtp2, int Woutp2,
                        matrix[double] outa3, matrix[double] outr3, 
                        matrix[double] outd3, matrix[double] maskd3,
                        matrix[double] outa4, matrix[double] probs)
        return (matrix[double] dW1, matrix[double] db1,
                matrix[double] dW2, matrix[double] db2,
                matrix[double] dW3, matrix[double] db3,
                matrix[double] dW4, matrix[double] db4) {

        Hf = 5  # filter height
        Wf = 5  # filter width
        stride = 1
        pad = 2  # For same dimensions, (Hf - stride) / 2

        F1 = 32  # num conv filters in conv1
        F2 = 64  # num conv filters in conv2

        # Compute data backward pass
        ## loss:
        dprobs = cross_entropy_loss::backward(probs, y)
        ## layer 4:  affine4 -> softmax
        douta4 = softmax::backward(dprobs, outa4)
        [doutd3, dW4, db4] = affine::backward(douta4, outr3, W4, b4)
        ## layer 3:  affine3 -> relu3 -> dropout
        doutr3 = dropout::backward(doutd3, outr3, 0.5, maskd3)
        douta3 = relu::backward(doutr3, outa3)
        [doutp2, dW3, db3] = affine::backward(douta3, outp2, W3, b3)
        ## layer 2: conv2 -> relu2 -> pool2
        doutr2 = max_pool2d::backward(doutp2, Houtp2, Woutp2, outr2, F2, Houtc2, Woutc2, Hf=2, Wf=2,
                                        strideh=2, stridew=2, padh=0, padw=0)
        doutc2 = relu::backward(doutr2, outc2)
        [doutp1, dW2, db2] = conv2d::backward(doutc2, Houtc2, Woutc2, outp1, W2, b2, F1,
                                                Houtp1, Woutp1, Hf, Wf, stride, stride, pad, pad)
        ## layer 1: conv1 -> relu1 -> pool1
        doutr1 = max_pool2d::backward(doutp1, Houtp1, Woutp1, outr1, F1, Houtc1, Woutc1, Hf=2, Wf=2,
                                        strideh=2, stridew=2, padh=0, padw=0)
        doutc1 = relu::backward(doutr1, outc1)
        [dX_batch, dW1, db1] = conv2d::backward(doutc1, Houtc1, Woutc1, X, W1, b1, C, Hin, Win,
                                                Hf, Wf, stride, stride, pad, pad)

}


gradients = function(list[unknown] model, list[unknown] hyperparams, 
matrix[double] features, matrix[double] labels)
    return (list[unknown] gradients) {
        
        loss_fcn = as.scalar(hyperparams["loss_fcn"])
        out_activation = as.scalar(model["activation"])
        cache = lenet_fw::lenetForwardPass(features, model)


        [outc1, Houtc1, Woutc1, outr1, outp1, 
        Houtp1, Woutp1, outc2, Houtc2, Woutc2, 
        outr2, outp2, Houtp2, Woutp2, 
        outa3, outr3, outd3, maskd3, outa4, probs] = lenet_fw::lenetForward(X_batch, C, Hin, Win, W1, b1, W2, b2, W3, b3, W4, b4)
        
        [dW1, db1, dW2, db2, dW3, db3, dW4, db4] = feed_backward(features, labels, model, cache, dout2)
        gradients = list(dW1, db1, dW2, db2)
    }

aggregation = function(list[unknown] model, list[unknown] hyperparams, list[unknown] gradients)
return (list[unknown] modelResult) {

    dW1 = as.matrix(gradients[1])
    db1 = as.matrix(gradients[2])
    dW2 = as.matrix(gradients[3])
    db2 = as.matrix(gradients[4])

    lr = as.scalar(hyperparams["lr"])

    W1 = sgd::update(as.matrix(model["W1"]), dW1, lr)
    b1 = sgd::update(as.matrix(model["b1"]), db1, lr)
    W2 = sgd::update(as.matrix(model["W2"]), dW2, lr)
    b2 = sgd::update(as.matrix(model["b2"]), db2, lr)

    modelResult = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=as.scalar(model["activation"]))
}