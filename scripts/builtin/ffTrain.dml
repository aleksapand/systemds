source("nn/layers/affine.dml") as affine
source("nn/layers/l1_loss.dml") as l1_loss
source("nn/layers/l2_loss.dml") as l2_loss
source("nn/layers/log_loss.dml") as log_loss
source("nn/layers/logcosh_loss.dml") as logcosh_loss
source("nn/layers/affineForwardPass.dml") as ff_pass
source("nn/layers/sigmoid.dml") as sigmoid
source("nn/layers/dropout.dml") as dropout
source("nn/optim/sgd.dml") as sgd
source("nn/layers/softmax.dml") as softmax
source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
source("nn/layers/relu.dml") as relu
source("nn/layers/leaky_relu.dml") as lrelu
source("nn/layers/tanh.dml") as tanh

source("nn/optim/sgd_nesterov.dml") as sgd_nesterov


m_ffTrain = function(Matrix[double] X, Matrix[double] Y, Integer batch_size=64, Integer epochs=20, 
    Double lr = 0.003, String out_activation = "sigmoid", String loss_fcn = "cel", Boolean param_server=FALSE, 
    Integer workers = 0, String utype = "ASP", String freq = "EPOCH", String mode = "LOCAL", Integer seed = -1) 
    return (List[unknown] model) {
    

    N = nrow(X) # number of samples, # y_rows = y_rows
    D = ncol(X) # number of features
    t = ncol(Y) # number of targets
    
    H1 = 64 # number of layer1 neurons

    [W1, b1] = affine::init(D, H1, seed)
    [W2, b2] = affine::init(H1, t, seed)


    ####################################################
    # Initialize SGD
    lr = lr  # learning rate
    mu = 0  # momentum
    decay = 0.99  # learning rate decay constant
    vW1 = sgd_nesterov::init(W1)  # optimizer momentum state for W_1
    vb1 = sgd_nesterov::init(b1)  # optimizer momentum state for b_1
    vW2 = sgd_nesterov::init(W2)  # optimizer momentum state for W_2
    vb2 = sgd_nesterov::init(b2)  # optimizer momentum state for b_2
    ##############################################################


    if (param_server) {
        model = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=out_activation)
        model = train_paramserv(model, X, Y, epochs, batch_size, lr, loss_fcn, 2, utype, freq, mode)
    } else {
        # TODO: Consider rest of division!
        iters = N / batch_size
        for (e in 1:epochs) {
            for(i in 1:iters) {

                X_batch = X[i:i+batch_size-1,]
                Y_batch = Y[i:i+batch_size-1,]

                layers = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=out_activation)
                cache = ff_pass::affineForwardPass(X_batch, layers)

                if (out_activation != "logits") {
                    loss = loss_forward(as.matrix(cache["outs2"]), Y_batch, loss_fcn)
                    dout2 = loss_backward(as.matrix(cache["outs2"]), Y_batch, loss_fcn)
                } else {
                    loss = loss_forward(as.matrix(cache["out2"]), Y_batch, loss_fcn)
                    dout2 = loss_backward(as.matrix(cache["out2"]), Y_batch, loss_fcn)
                }

                [dW1, db1, dW2, db2] = feed_backward(Y_batch, X_batch, layers, cache, dout2)

                # W1 = sgd::update(W1, dW1, lr)
                # b1 = sgd::update(b1, db1, lr)
                # W2 = sgd::update(W2, dW2, lr)
                # b2 = sgd::update(b2, db2, lr)

                ##############################################################
                # Optimize with SGD
                [W2, vW2] = sgd_nesterov::update(W2, dW2, lr, mu, vW2)
                [b2, vb2] = sgd_nesterov::update(b2, db2, lr, mu, vb2)
                [W1, vW1] = sgd_nesterov::update(W1, dW1, lr, mu, vW1)
                [b1, vb1] = sgd_nesterov::update(b1, db1, lr, mu, vb1)
                #######################################################

            }
                # Anneal momentum towards 0.999
                mu = mu + (0.999 - mu)/(1+epochs-e)
                # Decay learning rate
                lr = lr * decay
            print("Epoch: " + e + ", Loss: " + loss)
        }
        model = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=out_activation)
    }
}


feed_backward = function(Matrix[double] Y, Matrix[double] X, List[unknown] layers, List[unknown] cache, Matrix[double] dout) 
return(Matrix[double] dW1, Matrix[double] db1, Matrix[double] dW2, Matrix[double] db2)
{   p = 0.35 # dropout probability
    if (as.scalar(layers["activation"]) != "logits") {
        dout = apply_activation_backward(dout, as.matrix(cache["out2"]), as.scalar(layers["activation"]))
    } 
    # Layer 2
    [doutd1, dW2, db2] = affine::backward(dout, as.matrix(cache["outd1"]), as.matrix(layers["W2"]), as.matrix(layers["b2"]))
    # Layer 1
    doutr1 = dropout::backward(doutd1, as.matrix(cache["outr1"]), p, as.matrix(cache["maskd1"]))
    dout1 = sigmoid::backward(doutr1, as.matrix(cache["out1"]))
    [dx, dW1, db1] = affine::backward(dout1, X, as.matrix(layers["W1"]), as.matrix(layers["b1"]))
}

apply_activation_backward = function(Matrix[double] dout, Matrix[double] X, String activation) 
return (Matrix[double] out) {
    
    if(activation == "sigmoid") {
        out = sigmoid::backward(dout, X)
    } else if (activation == "relu") {
        out = relu::backward(dout, X)
    } else if (activation == "lrelu") {
        out = lrelu::backward(dout, X)
    } else if (activation == "tanh") {
        out = tanh::backward(dout, X)
    } else if (activation == "softmax") {
        out = softmax::backward(dout, X)
    }
}

loss_forward = function(Matrix[double] prediction, Matrix[double] target, String loss_fcn)
return(Double loss) {
    if (loss_fcn == "l1") {
        loss = l1_loss::forward(prediction, target)
    } else if(loss_fcn == "l2") {
        loss = l2_loss::forward(prediction, target)
    } else if(loss_fcn == "log_loss") {
        loss = log_loss::forward(prediction, target)
    } else if(loss_fcn == "logcosh_loss") {
        loss = logcosh_loss::forward(prediction, target)
    } else {
        loss = cross_entropy_loss::forward(prediction, target)
    }
}

loss_backward = function(Matrix[double] prediction, Matrix[double] target, String loss_fcn)
return(Matrix[Double] dout) {
    if (loss_fcn == "l1") {
        dout = l1_loss::backward(prediction, target)
    } else if(loss_fcn == "l2") {
        dout = l2_loss::backward(prediction, target)
    } else if(loss_fcn == "log_loss") {
        dout = log_loss::backward(prediction, target)
    } else if(loss_fcn == "logcosh_loss") {
        dout = logcosh_loss::backward(prediction, target)
    } else {
        dout = cross_entropy_loss::backward(prediction, target)
    }
}

gradients = function(list[unknown] model, list[unknown] hyperparams, 
matrix[double] features, matrix[double] labels)
    return (list[unknown] gradients) {
        
        loss_fcn = as.scalar(hyperparams["loss_fcn"])
        out_activation = as.scalar(model["activation"])
        cache = ff_pass::affineForwardPass(features, model)

        if (out_activation != "logits") {
            loss = loss_forward(as.matrix(cache["outs2"]), labels, loss_fcn)
            dout2 = loss_backward(as.matrix(cache["outs2"]), labels, loss_fcn)
        } else {
            loss = loss_forward(as.matrix(cache["out2"]), labels, loss_fcn)
            dout2 = loss_backward(as.matrix(cache["out2"]), labels, loss_fcn)
        }
        
        [dW1, db1, dW2, db2] = feed_backward(labels, features, model, cache, dout2)
        gradients = list(dW1, db1, dW2, db2)
    }

aggregation = function(list[unknown] model, list[unknown] hyperparams, list[unknown] gradients)
return (list[unknown] modelResult) {

    dW1 = as.matrix(gradients[1])
    db1 = as.matrix(gradients[2])
    dW2 = as.matrix(gradients[3])
    db2 = as.matrix(gradients[4])

    lr = as.scalar(hyperparams["lr"])

    W1 = sgd::update(as.matrix(model["W1"]), dW1, lr)
    b1 = sgd::update(as.matrix(model["b1"]), db1, lr)
    W2 = sgd::update(as.matrix(model["W2"]), dW2, lr)
    b2 = sgd::update(as.matrix(model["b2"]), db2, lr)

    modelResult = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=as.scalar(model["activation"]))
}


train_paramserv = function(List[unknown] model, matrix[Double] X, matrix[Double] y,
Integer epochs, Integer batch_size, Double learning_rate, String loss_function, Integer workers,
String utype, String freq, String mode)
return (list[unknown] model_trained) {

    # N = nrow(X)
    # iters = N / batch_size
    # for (e in 1:epochs) {
    #     for(i in 1:iters) {

    #         X_batch = X[i:i+batch_size-1,]
    #         Y_batch = y[i:i+batch_size-1,]

    #         params = list(lr=0.001, loss_fcn="cel")
    #         grad = gradients(model, params, X_batch, Y_batch)
    #         model = aggregation(model, params, grad)
    #     }
    # }
    # model_trained = model   
    
    params = list(lr=0.001, loss_fcn="cel")
    model_trained = paramserv(model=model, features=X, labels=y, upd="gradients", agg="aggregation", epochs=1, hyperparams=params)
    
}