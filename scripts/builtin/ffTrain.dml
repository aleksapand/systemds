#-------------------------------------------------------------
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
#-------------------------------------------------------------
#
# This builtin function trains simple feed-forward neural network. The architecture of the
# networks is: affine1 -> relu -> dropout -> affine2 -> configurable output activation function.
#
# INPUT PARAMETERS:
# --------------------------------------------------------------------------------------------
# NAME            TYPE            DEFAULT     MEANING
# --------------------------------------------------------------------------------------------
# X               Matrix[double]    ---       Training data
# Y               Matrix[Double]    ---       Labels/Target values
# batch_size      Integer           64        Batch size
# epochs          Integer           20        Number of epochs
# learning_rate   Double            0.003     Learning rate
# out_activation  String            ---       User specified ouptut activation function. Possible values:
# "sigmoid", "relu", "lrelu", "tanh", "softmax", "logits" (no activation).
# loss_fcn        String            ---       User specified loss function. Possible values:
# "l1", "l2", "log_loss", "logcosh_loss", "cel" (cross-entropy loss).
# param_server    Boolean           FALSE     Flag which indicates if paramserv is to be used
# workers         Integer           0         paramserv number of workers
# utype           String            "ASP"     paramserv update strategy
# freq            String            "EPOCH"   paramserv model updating frequency
# mode            String            "LOCAL"   paramserv execution backend for data partitioning and param exec
# seed            Integer           -1        Seed for model initialization
# verbose         Boolean           FALSE     Flag which indicates if function should print to stdout
# --------------------------------------------------------------------------------------------
# OUTPUT: 
# model           List[unknown]     ---       Trained model which can be used in ffPredict
#


source("nn/layers/affine.dml") as affine
source("nn/layers/dropout.dml") as dropout
source("nn/layers/feedForward.dml") as ff_pass

# Supported loss functions by the model
source("nn/layers/l1_loss.dml") as l1_loss
source("nn/layers/l2_loss.dml") as l2_loss
source("nn/layers/log_loss.dml") as log_loss
source("nn/layers/logcosh_loss.dml") as logcosh_loss
source("nn/layers/cross_entropy_loss.dml") as cel

# Supported activation functions by the model
source("nn/layers/sigmoid.dml") as sigmoid
source("nn/layers/relu.dml") as relu
source("nn/layers/leaky_relu.dml") as lrelu
source("nn/layers/tanh.dml") as tanh
source("nn/layers/softmax.dml") as softmax

source("nn/optim/sgd_nesterov.dml") as sgd_nesterov
# Used in paramserv
source("nn/optim/sgd.dml") as sgd


m_ffTrain = function(Matrix[double] X, Matrix[double] Y, Integer batch_size=64, 
Integer epochs=20, Double learning_rate=0.003, String out_activation, 
String loss_fcn, Boolean param_server=FALSE, Integer workers=0, 
String utype="ASP", String freq="EPOCH", String mode="LOCAL", 
Integer seed=-1, Boolean verbose=FALSE) 
  return (List[unknown] model) {
    
  N = nrow(X) # number of samples
  D = ncol(X) # number of features
  t = ncol(Y) # number of targets
  
  H1 = 128 # number of layer1 neurons

  # Init layers
  [W1, b1] = affine::init(D, H1, seed)
  [W2, b2] = affine::init(H1, t, seed)

  # Initialize SGD
  lr = learning_rate  
  mu = 0 
  decay = 0.99 
  vW1 = sgd_nesterov::init(W1)
  vb1 = sgd_nesterov::init(b1)
  vW2 = sgd_nesterov::init(W2) 
  vb2 = sgd_nesterov::init(b2)

  if (param_server) {
    # Output activation function is stored in the list together
    # with the layers since different activation functions might
    # be specified (or none). When forward/backward pass is 
    # performed, member "activation" of the list model is used 
    # such that appropriate functions are applied. This is
    # advantage for the user which does not have to pass model 
    # and activation function as two arguments in predict method.
    model = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=out_activation)
    model = train_paramserv(model, X, Y, epochs, batch_size, lr, loss_fcn, 2, utype, freq, mode)
  } else {
    iters = N / batch_size
    
    for (e in 1:epochs) {
      for(i in 1:iters) {
        X_batch = X[i:i+batch_size-1,]
        Y_batch = Y[i:i+batch_size-1,]

        layers = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=out_activation)
        cache = ff_pass::feedForward(X_batch, layers)

        # Distinguish two cases when loss is calculated from the raw output
        # or from the output of the activation function.
        if (out_activation != "logits") {
          loss = loss_forward(as.matrix(cache["outs2"]), Y_batch, loss_fcn)
          dout2 = loss_backward(as.matrix(cache["outs2"]), Y_batch, loss_fcn)
        } else {
          loss = loss_forward(as.matrix(cache["out2"]), Y_batch, loss_fcn)
          dout2 = loss_backward(as.matrix(cache["out2"]), Y_batch, loss_fcn)
        }

        [dW1, db1, dW2, db2] = feed_backward(Y_batch, X_batch, layers, cache, dout2)

        [W2, vW2] = sgd_nesterov::update(W2, dW2, lr, mu, vW2)
        [b2, vb2] = sgd_nesterov::update(b2, db2, lr, mu, vb2)
        [W1, vW1] = sgd_nesterov::update(W1, dW1, lr, mu, vW1)
        [b1, vb1] = sgd_nesterov::update(b1, db1, lr, mu, vb1)
      }
      
      mu = mu + (0.999 - mu)/(1+epochs-e)
      lr = lr * decay
      
      if(verbose) {
        print("Epoch: " + e + ", Loss: " + loss)
      }
    }
    model = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=out_activation)
  }
}


feed_backward = function(Matrix[double] Y, Matrix[double] X, List[unknown] layers, List[unknown] cache, Matrix[double] dout) 
  return(Matrix[double] dW1, Matrix[double] db1, Matrix[double] dW2, Matrix[double] db2){
  p = 0.35 # dropout probability
  
  if (as.scalar(layers["activation"]) != "logits") {
    dout = apply_activation_backward(dout, as.matrix(cache["out2"]), as.scalar(layers["activation"]))
  } 
  # Layer 2
  [doutd1, dW2, db2] = affine::backward(dout, as.matrix(cache["outd1"]), as.matrix(layers["W2"]), as.matrix(layers["b2"]))
  # Layer 1
  doutr1 = dropout::backward(doutd1, as.matrix(cache["outr1"]), p, as.matrix(cache["maskd1"]))
  dout1 = sigmoid::backward(doutr1, as.matrix(cache["out1"]))
  [dx, dW1, db1] = affine::backward(dout1, X, as.matrix(layers["W1"]), as.matrix(layers["b1"]))
}

apply_activation_backward = function(Matrix[double] dout, Matrix[double] X, String activation) 
  return (Matrix[double] out) { 

  if(activation == "sigmoid") {
    out = sigmoid::backward(dout, X)
  } else if (activation == "relu") {
    out = relu::backward(dout, X)
  } else if (activation == "lrelu") {
    out = lrelu::backward(dout, X)
  } else if (activation == "tanh") {
    out = tanh::backward(dout, X)
  } else if (activation == "softmax") {
    out = softmax::backward(dout, X)
  }
}

loss_forward = function(Matrix[double] prediction, Matrix[double] target, String loss_fcn)
  return(Double loss) {

  if (loss_fcn == "l1") {
    loss = l1_loss::forward(prediction, target)
  } else if(loss_fcn == "l2") {
    loss = l2_loss::forward(prediction, target)
  } else if(loss_fcn == "log_loss") {
    loss = log_loss::forward(prediction, target)
  } else if(loss_fcn == "logcosh_loss") {
    loss = logcosh_loss::forward(prediction, target)
  } else {
    loss = cel::forward(prediction, target)
  }
}

loss_backward = function(Matrix[double] prediction, Matrix[double] target, String loss_fcn)
  return(Matrix[Double] dout) {

  if (loss_fcn == "l1") {
    dout = l1_loss::backward(prediction, target)
  } else if(loss_fcn == "l2") {
    dout = l2_loss::backward(prediction, target)
  } else if(loss_fcn == "log_loss") {
    dout = log_loss::backward(prediction, target)
  } else if(loss_fcn == "logcosh_loss") {
    dout = logcosh_loss::backward(prediction, target)
  } else {
    dout = cel::backward(prediction, target)
  }
}

gradients = function(list[unknown] model, list[unknown] hyperparams, 
matrix[double] features, matrix[double] labels)
  return (list[unknown] gradients) {
        
  loss_fcn = as.scalar(hyperparams["loss_fcn"])
  out_activation = as.scalar(model["activation"])
  cache = ff_pass::feedForward(features, model)

  # Distinguish two cases when loss is calculated from the raw output
  # or from the output of the activation function.
  if (out_activation != "logits") {
    loss = loss_forward(as.matrix(cache["outs2"]), labels, loss_fcn)
    dout2 = loss_backward(as.matrix(cache["outs2"]), labels, loss_fcn)
  } else {
    loss = loss_forward(as.matrix(cache["out2"]), labels, loss_fcn)
    dout2 = loss_backward(as.matrix(cache["out2"]), labels, loss_fcn)
  }
  
  [dW1, db1, dW2, db2] = feed_backward(labels, features, model, cache, dout2)
  gradients = list(dW1, db1, dW2, db2)
}

aggregation = function(list[unknown] model, list[unknown] hyperparams, list[unknown] gradients)
  return (list[unknown] modelResult) {

  dW1 = as.matrix(gradients[1])
  db1 = as.matrix(gradients[2])
  dW2 = as.matrix(gradients[3])
  db2 = as.matrix(gradients[4])

  lr = as.scalar(hyperparams["lr"])

  W1 = sgd::update(as.matrix(model["W1"]), dW1, lr)
  b1 = sgd::update(as.matrix(model["b1"]), db1, lr)
  W2 = sgd::update(as.matrix(model["W2"]), dW2, lr)
  b2 = sgd::update(as.matrix(model["b2"]), db2, lr)

  modelResult = list(W1=W1, b1=b1, W2=W2, b2=b2, activation=as.scalar(model["activation"]))
}


train_paramserv = function(List[unknown] model, matrix[Double] X, matrix[Double] y,
Integer epochs, Integer batch_size, Double learning_rate, String loss_function, 
Integer workers, String utype, String freq, String mode)
  return (list[unknown] model_trained) {

  # Code used for trying out aggregation and gradients methods for paramserv-
  # I kept it included since error with paramserv is still unresolved.
  # N = nrow(X)
  # iters = N / batch_size
  # for (e in 1:epochs) {
  #     for(i in 1:iters) {

  #         X_batch = X[i:i+batch_size-1,]
  #         Y_batch = y[i:i+batch_size-1,]

  #         params = list(lr=0.001, loss_fcn="cel")
  #         grad = gradients(model, params, X_batch, Y_batch)
  #         model = aggregation(model, params, grad)
  #     }
  # }
  # model_trained = model   
  
  params = list(lr=learning_rate, loss_fcn=loss_function)
  model_trained = paramserv(model=model, features=X, labels=y, upd="gradients", agg="aggregation", 
  epochs=epochs, hyperparams=params, mode=mode, utype=utype, freq=freq, batchsize=batch_size, k=workers, 
  checkpointing="NONE")
}
