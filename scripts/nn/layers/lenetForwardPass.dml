# Imports
source("nn/layers/affine.dml") as affine
source("nn/layers/conv2d_builtin.dml") as conv2d
source("nn/layers/dropout.dml") as dropout
source("nn/layers/max_pool2d_builtin.dml") as max_pool2d
source("nn/layers/relu.dml") as relu
source("nn/layers/softmax.dml") as softmax



lenetForward = function(Matrix[Double] X, Integer C, Integer Hin, Integer Win,
                        Matrix[Double] W1, Matrix[Double] b1,
                        Matrix[Double] W2, Matrix[Double] b2,
                        Matrix[Double] W3, Matrix[Double] b3,
                        Matrix[Double] W4, Matrix[Double] b4) 
return (Matrix[Double] outc1, Integer Houtc1, Integer Woutc1, Matrix[Double] outr1,
        Matrix[Double] outp1, Integer Houtp1, Integer Woutp1,
        Matrix[Double] outc2, Integer Houtc2, Integer Woutc2, Matrix[Double] outr2,
        Matrix[Double] outp2, Integer Houtp2, Integer Woutp2,
        Matrix[Double] outa3, Matrix[Double] outr3, 
        Matrix[Double] outd3, Matrix[Double] maskd3,
        Matrix[Double] outa4, Matrix[Double] probs) {

    Hf = 5  # filter height
    Wf = 5  # filter width
    stride = 1
    pad = 2  # For same dimensions, (Hf - stride) / 2

    F1 = nrow(W1)  # num conv filters in conv1
    F2 = nrow(W2)  # num conv filters in conv2
 
    # Compute forward pass
    ## layer 1: conv1 -> relu1 -> pool1
    [outc1, Houtc1, Woutc1] = conv2d::forward(X, W1, b1, C, Hin, Win, Hf, Wf,
                                            stride, stride, pad, pad)
    outr1 = relu::forward(outc1)
    [outp1, Houtp1, Woutp1] = max_pool2d::forward(outr1, F1, Houtc1, Woutc1, Hf=2, Wf=2,
                                                strideh=2, stridew=2, padh=0, padw=0)
    ## layer 2: conv2 -> relu2 -> pool2
    [outc2, Houtc2, Woutc2] = conv2d::forward(outp1, W2, b2, F1, Houtp1, Woutp1, Hf, Wf,
                                            stride, stride, pad, pad)
    outr2 = relu::forward(outc2)
    [outp2, Houtp2, Woutp2] = max_pool2d::forward(outr2, F2, Houtc2, Woutc2, Hf=2, Wf=2,
                                                strideh=2, stridew=2, padh=0, padw=0)
    ## layer 3:  affine3 -> relu3 -> dropout
    outa3 = affine::forward(outp2, W3, b3)
    outr3 = relu::forward(outa3)
    [outd3, maskd3] = dropout::forward(outr3, 0.5, -1)
    ## layer 4:  affine4 -> softmax
    outa4 = affine::forward(outd3, W4, b4)
    probs = softmax::forward(outa4)

}