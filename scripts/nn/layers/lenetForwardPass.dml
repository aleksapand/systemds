# Imports
source("nn/layers/affine.dml") as affine
source("nn/layers/conv2d_builtin.dml") as conv2d
source("nn/layers/dropout.dml") as dropout
source("nn/layers/max_pool2d_builtin.dml") as max_pool2d
source("nn/layers/relu.dml") as relu
source("nn/layers/softmax.dml") as softmax



lenetForward = function(matrix[double] X, int C, int Hin, int Win,
                        matrix[double] W1, matrix[double] b1,
                        matrix[double] W2, matrix[double] b2,
                        matrix[double] W3, matrix[double] b3,
                        matrix[double] W4, matrix[double] b4) 
return (matrix[double] outc1, int Houtc1, int Woutc1, matrix[double] outr1,
        matrix[double] outp1, int Houtp1, int Woutp1,
        matrix[double] outc2, int Houtc2, int Woutc2, matrix[double] outr2,
        matrix[double] outp2, int Houtp2, int Woutp2,
        matrix[double] outa3, matrix[double] outr3, 
        matrix[double] outd3, matrix[double] maskd3,
        matrix[double] outa4, matrix[double] probs) {

    Hf = 5  # filter height
    Wf = 5  # filter width
    stride = 1
    pad = 2  # For same dimensions, (Hf - stride) / 2

    F1 = nrow(W1)  # num conv filters in conv1
    F2 = nrow(W2)  # num conv filters in conv2
 
    # Compute forward pass
    ## layer 1: conv1 -> relu1 -> pool1
    [outc1, Houtc1, Woutc1] = conv2d::forward(X, W1, b1, C, Hin, Win, Hf, Wf,
                                            stride, stride, pad, pad)
    outr1 = relu::forward(outc1)
    [outp1, Houtp1, Woutp1] = max_pool2d::forward(outr1, F1, Houtc1, Woutc1, Hf=2, Wf=2,
                                                strideh=2, stridew=2, padh=0, padw=0)
    ## layer 2: conv2 -> relu2 -> pool2
    [outc2, Houtc2, Woutc2] = conv2d::forward(outp1, W2, b2, F1, Houtp1, Woutp1, Hf, Wf,
                                            stride, stride, pad, pad)
    outr2 = relu::forward(outc2)
    [outp2, Houtp2, Woutp2] = max_pool2d::forward(outr2, F2, Houtc2, Woutc2, Hf=2, Wf=2,
                                                strideh=2, stridew=2, padh=0, padw=0)
    ## layer 3:  affine3 -> relu3 -> dropout
    outa3 = affine::forward(outp2, W3, b3)
    outr3 = relu::forward(outa3)
    [outd3, maskd3] = dropout::forward(outr3, 0.5, -1)
    ## layer 4:  affine4 -> softmax
    outa4 = affine::forward(outd3, W4, b4)
    probs = softmax::forward(outa4)

}