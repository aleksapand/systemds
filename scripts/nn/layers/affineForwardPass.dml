source("nn/layers/sigmoid.dml") as sigmoid
source("nn/layers/leaky_relu.dml") as lrelu
source("nn/layers/relu.dml") as relu
source("nn/layers/tanh.dml") as tanh
source("nn/layers/affine.dml") as affine
source("nn/layers/dropout.dml") as dropout
source("nn/layers/softmax.dml") as softmax

affineForwardPass = function(Matrix[double] X, List[unknown] layers) 
return(List[unknown] cache) {
    p = 0.35 # dropout probability

    # layer 1
    out1 = affine::forward(X, as.matrix(layers["W1"]), as.matrix(layers["b1"]))
    outr1 = relu::forward(out1)
    [outd1, maskd1] = dropout::forward(outr1, p, -1)
    # layer 2
    out2 = affine::forward(outd1, as.matrix(layers["W2"]), as.matrix(layers["b2"]))
    
    if (as.scalar(layers["activation"]) == "logits") {
        cache = list(out1=out1, outr1=outr1, outd1=outd1, maskd1=maskd1, out2=out2)
    } else {
        outs2 = apply_activation(out2, as.scalar(layers["activation"]))
        cache = list(out1=out1, outr1=outr1, outd1=outd1, maskd1=maskd1, out2=out2, outs2=outs2)
    }
}

apply_activation = function(Matrix[double] input, String activation) 
return (Matrix[double] out) {
    
    if(activation == "sigmoid") {
        out = sigmoid::forward(input)
    } else if (activation == "relu") {
        out = relu::forward(input)
    } else if (activation == "lrelu") {
        out = lrelu::forward(input)
    } else if (activation == "tanh") {
        out = tanh::forward(input)
    } else if (activation == "softmax") {
        out = softmax::forward(input)
    }
}